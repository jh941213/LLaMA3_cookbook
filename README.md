# LLaMA3_Cookbook ðŸ¦™âœ¨
ì•ˆê²½ì„ ì“´ ê·€ì—¬ìš´ ë¼ë§ˆë“¤ > _ <
![alt text](ì˜ˆì‹œ-1.png)

LLaMA3 (Large Language Model by META AI)ëŠ” AI ê¸°ìˆ ì˜ ìµœì „ì„ ì—ì„œ í™œì•½í•˜ëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ìž…ë‹ˆë‹¤. ðŸŒŸ ì´ ì €ìž¥ì†ŒðŸ“ëŠ” LLaMA3ë¥¼ í™œìš©í•˜ì—¬ ë‹¤ì–‘í•œ í”„ë¡œì íŠ¸ðŸš€ë¥¼ ì‹œìž‘í•˜ëŠ” ë° í•„ìš”í•œ ì •ë³´ë¥¼ ì œê³µí•˜ê³ ìž í•©ë‹ˆë‹¤.


## [ðŸ“œIntroducing Meta Llama 3: The most capable openly available LLM to date ë¦¬ë·° ](https://hyun941213.tistory.com/entry/Introducing-Meta-Llama-3-The-most-capable-openly-available-LLM-to-date-%EB%A6%AC%EB%B7%B0)

## ê³µì‹ ì›¹ì‚¬ì´íŠ¸ ë° ì •ë³´ ðŸŒ
- [ê³µì‹ í™ˆíŽ˜ì´ì§€ ðŸ ](https://llama.meta.com/)
- [ì ‘ê·¼ ìš”ì²­ ðŸ“¬](https://llama.meta.com/llama-downloads/)
- [Meta Llama Model ì¹´ë“œ ðŸŽ´](https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3)
- [Kaggle Meta ðŸ…](https://www.kaggle.com/organizations/metaresearch/models)

## âš¡ï¸ Cloud API 
### API í˜¸ì¶œ ê°€ëŠ¥ ðŸ”Œ
| ì´ë¦„          | ì„¤ëª…                                                              | ë§í¬ |
|--------------|-------------------------------------------------------------------|-----|
| Grok         | ê³ ì„±ëŠ¥ AI Chipì„ í†µí•´ LLaMA3 ì¸í¼ëŸ°ìŠ¤ ë° API í˜¸ì¶œ ê°€ëŠ¥            | [Grok ðŸŒ](https://console.groq.com/playground) |
| AWS          | Bedrockì—ì„œ LLaMA ì§€ì›, í˜„ìž¬ëŠ” Llama2ë§Œ ì‚¬ìš© ê°€ëŠ¥                  | [AWS ðŸŒ](https://aws.amazon.com/ko/bedrock/) |
| Azure        | Microsoft Azureì—ì„œ 8B/70B ëª¨ë¸ ì§€ì›, Azure Marketplaceì—ì„œ ê²€ìƒ‰ ê°€ëŠ¥ | [Azure ðŸŒ](https://azuremarketplace.microsoft.com/en-us/marketplace/apps/metagenai.meta-llama-3-8b-chat-offer?tab=overview)|
| GCP| Google Cloud Vertax AI ì—ì„œ ì‚¬ìš© ê°€ëŠ¥ | [GCP ðŸŒ](https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama3?_ga=2.164398141.-384541959.1712575317)
| together.ai  | Llama2, CodeLlama, Llama3 8b/70b ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš© ê°€ëŠ¥                | [together.ai ðŸŒ](https://www.together.ai/) |
| replicate    | Llama3 API ì§€ì› (Node.js, Python, HTTP)                            | [replicate ðŸŒ](https://replicate.com/blog/run-llama-3-with-an-api) |
| llama AI    | Llama3 8B/70B ì§€ì›, ë‹¤ë¥¸ OpenLLM ì§€ì›                         | [llama AI ðŸŒ](https://www.llama-api.com/) |
| Meta AI(github) | meta AI api ì—°ê²° | [MetaAI ðŸŒ](https://github.com/Strvm/meta-ai-api?tab=readme-ov-file)|


## ðŸ¤– ì¸í¼ëŸ°ìŠ¤ (Inference) ðŸ§ 

### ì¸í¼ëŸ°ìŠ¤ í”Œëž«í¼ ðŸ–¥ï¸
| í”Œëž«í¼ ì´ë¦„   | ì„¤ëª…                               | ë§í¬ |
|--------------|------------------------------------|-----|
| HuggingFace  | Llama 8B ëª¨ë¸                      | [Link ðŸŒ](https://meta-llama/Meta-Llama-3-8B) |
| HuggingFace  | Llama 70B ëª¨ë¸                     | [Link ðŸŒ](https://huggingface.co/meta-llama/Meta-Llama-3-70B) |
| HuggingFace  | Llama 8B Instruct ëª¨ë¸             | [Link ðŸŒ](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) |
| HuggingFace  | Llama 70B Instruct ëª¨ë¸            | [Link ðŸŒ](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct) |
| HuggingFace  | Llama Guard-2-8B(ì •ì±…ëª¨ë¸)         | [Link ðŸŒ](https://huggingface.co/meta-llama/Meta-Llama-Guard-2-8B) |
| HuggingFace | Llama 8B KO (made beomi) | [Link ðŸŒ](https://huggingface.co/beomi/Llama-3-Open-Ko-8B-preview)|
| Ollama       | ë‹¤ì–‘í•œ ê²½ëŸ‰í™”ëœ Llama3 ëª¨ë¸ ì¸í¼ëŸ°ìŠ¤ ê°€ëŠ¥ | [Link ðŸŒ](https://ollama.com/library/llama3) |

### HuggingFace ëª¨ë¸ ðŸ¥
| ì´ë¦„   | ì„¤ëª…                               | ë§í¬ |
|--------------|----------------------------|-----|
|cognitivecomputations/dolphin-2.9-llama3-8b| ë¬´ê²€ì—´ íŒŒì¸íŠœë‹ | [Link ðŸŒ](https://huggingface.co/cognitivecomputations/dolphin-2.9-llama3-8b)|
|McGill-NLP/Llama-3-8B-Web| ì œë¡œìƒ· ì¸í„°ë„· ë§í¬ ì„ íƒ ëŠ¥ë ¥ |[Link ðŸŒ](https://huggingface.co/McGill-NLP/Llama-3-8B-Web)
## ðŸ’¬ Chat Interface (Related Information) ðŸ’»
| ì´ë¦„              | ë§í¬ |
|-----------------|-----|
| HuggingChat     | [Link ðŸŒ](https://huggingface.co/chat/) |
| Groq            | [Link ðŸŒ](https://groq.com/) |
| together.ai     | [Link ðŸŒ](https://www.together.ai/) |
| replicate Llama chat(local) | [Link ðŸŒ](https://github.com/replicate/llama-chat)|
| perplexity.ai(ê²½ëŸ‰í™”ëª¨ë¸) | [Link ðŸŒ](https://labs.perplexity.ai/)|
|openrouter.ai| [Link ðŸŒ](https://openrouter.ai/playground?models=meta-llama/llama-3-70b-instruct)|
| MetaAI (í•œêµ­ ì‚¬ìš©ë¶ˆê°€)|[Link ðŸŒ](https://www.meta.ai/)|
| Morphic(ë©€í‹°ëª¨ë‹¬ì œê³µ) | [Link ðŸŒ](https://www.morphic.sh/)|

## LLaMA Framework  ðŸ“˜
| ì´ë¦„       |ìœ í˜•| ë§í¬ 
|----------|-----|-----|
| Langchain | RAG | [Link ðŸŒ](https://www.langchain.com/) |
| llamaindex| RAG | [Link ðŸŒ](https://www.llamaindex.ai/) |
| llama.cpp| convert | [Link ðŸŒ](https://github.com/ggerganov/llama.cpp) |

## ðŸ› ï¸ íŒŒì¸íŠœë‹ (Fine-tuning) ðŸ”§
| ì´ë¦„      | ë§í¬ |
|---------|-----|
| Meta | [Link ðŸŒ](https://llama.meta.com/docs/how-to-guides/fine-tuning/)
| torchrune| [Link ðŸŒ](https://github.com/pytorch/torchtune)|
| LLaMAFactory| [Link ðŸŒ](https://github.com/hiyouga/LLaMA-Factory)|
|axolotl| [Link ðŸŒ](https://github.com/OpenAccess-AI-Collective/axolotl)|



## LLAMA3_Cookbook ðŸ‘©â€ðŸ³
| ì •ë³´                               | ë§í¬ |
|----------------------------------|-----|
| í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê°€ì´ë“œ           | [Link ðŸŒ](https://www.promptingguide.ai/tools)|
| WEB UI ë¥¼ ì‚¬ìš©í•´ì„œ llama3 ì‚¬ìš© | [Link ðŸŒ](https://dev.to/timesurgelabs/how-to-run-llama-3-locally-with-ollama-and-open-webui-297d) |
| API with Ollama, LangChain and ChromaDB with Flask API and PDF upload | [Link ðŸŒ](https://www.youtube.com/watch?v=7VAs22LC7WE) |
| ë§¥ë¶ê´€ë ¨ Llama íŠœë‹ ë° ì¸í¼ëŸ°ìŠ¤ ê°€ì´ë“œ | [Link ðŸŒ](https://itnext.io/step-by-step-guide-to-running-latest-llm-model-meta-llama-3-on-apple-silicon-macs-m1-m2-or-m3-b9424ada6840) | 
| Fine-tune Llama 3 with ORPO | [Link ðŸŒ](https://huggingface.co/blog/mlabonne/orpo-llama-3)|
| Qlora_aplaca_llama3 finetune | [Link ðŸŒ](https://colab.research.google.com/drive/1mPw6P52cERr93w3CMBiJjocdTnyPiKTX#scrollTo=6bZsfBuZDeCL)
| fully local RAG agents with LLama3 | [Link ðŸŒ](https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_rag_agent_llama3_local.ipynb) |
| RAG Chatbot LLama3(HF) | [Link ðŸŒ](https://huggingface.co/blog/not-lain/rag-chatbot-using-llama3) |
| llama index RAG llama3 | [Link ðŸŒ](https://lightning.ai/lightning-ai/studios/rag-using-llama-3-by-meta-ai) |
| ollama RAG + UI(Gradio) | [Link ðŸŒ](https://mer.vin/2024/04/llama-3-rag-using-ollama/) |

## LLM Dataset ðŸ—‚ï¸
| ì •ë³´                               | ë§í¬ |
|----------------------------------|-----|


## LLM skills ðŸ“Œ
| ì •ë³´                               | ë§í¬ |
|----------------------------------|-----|
|FSDP+QLORA finetunning | [Link ðŸŒ](https://github.com/AnswerDotAI/fsdp_qlora)|


## MAC vs 4090 ë¹„êµ ðŸ–¥ï¸ðŸ†šðŸ–¥ï¸
| í•­ëª©          | M3 Max                              | M1 Pro                    | RTX 4090                                    |
|---------------|-------------------------------------|---------------------------|---------------------------------------------|
| CPU ì½”ì–´      | 16ì½”ì–´                              | 10ì½”ì–´                    | 16ì½”ì–´ AMD                                  |
| ë©”ëª¨ë¦¬        | 128GB                               | 16GB    /32GB                  | 32GB                                        |
| GPU ë©”ëª¨ë¦¬    | 16ì½”ì–´ CPU ë° 40ì½”ì–´ GPU, 400GB/s ë©”ëª¨ë¦¬ ëŒ€ì—­í­                           | 10ì½”ì–´ CPU(ì„±ëŠ¥ ì½”ì–´ 8ê°œ ë° íš¨ìœ¨ ì½”ì–´ 2ê°œ) 16ì½”ì–´ GPU 200GB/s ë©”ëª¨ë¦¬ ëŒ€ì—­í­                   | 24GB                                        |
| **ëª¨ë¸ 7B**   | - ëª¨ë“  ì»´í“¨í„°ì—ì„œ ìž˜ ìž‘ë™           | - ëª¨ë“  ì»´í“¨í„°ì—ì„œ ìž˜ ìž‘ë™ | - ëª¨ë“  ì»´í“¨í„°ì—ì„œ ìž˜ ìž‘ë™,ì„±ëŠ¥ M3 Maxì™€ ìœ ì‚¬                  |
| **ëª¨ë¸ 13B**  | - ì„±ëŠ¥ ì¢‹ìŒ                         | - ì„¸ ë²ˆì§¸ë¡œ ì„±ëŠ¥ ì¢‹ìŒ     | - ê°€ìž¥ ì„±ëŠ¥ ì¢‹ìŒ                            |
| **ëª¨ë¸ 70B**  | - ë¹ ë¥´ê²Œ ì‹¤í–‰, 128GB ë©”ëª¨ë¦¬ í™œìš©    | - 16GB ë¶€ì¡±, í¬ëž˜ì‹œ ë° ìž¬ë¶€íŒ… | - GPUì—ì„œ ì‹¤í–‰ ë¶ˆê°€, CPUì—ì„œ ë§¤ìš° ëŠë¦¬ê²Œ ì‹¤í–‰ |
| ê²½ëŸ‰í™” ì—¬ë¶€   | ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•˜ë‹¤ë©´ í•˜ì§€ ì•Šì•„ë„ ë¨ | í•´ì•¼í•  ë“¯ í•¨ | Quantization íƒ€í˜‘ì„ í•´ì•¼í•¨ |
| ì „ë ¥ ì†Œëª¨      | 65W                                |                           | 250-300W                                    |
| ê°€ê²© ëŒ€ë¹„ ì„±ëŠ¥ | ìš°ìˆ˜ ($4600)                       |                           | ë¹„êµì  ë‚®ìŒ ($6000 for A6000 GPU)           |

## ðŸ™Œ ê¸°ì—¬í•˜ê¸° ðŸ’–
ì´ ì €ìž¥ì†Œì— ê¸°ì—¬í•˜ê³  ì‹¶ìœ¼ì‹ ê°€ìš”? [Issues](https://github.com/your-github/LLaMA3_Recipes/issues)ì— ìžìœ ë¡­ê²Œ ì˜ê²¬ì„ ë‚¨ê¸°ê±°ë‚˜ Pull Requestë¥¼ ë³´ë‚´ì£¼ì„¸ìš”. ëª¨ë“  ì¢…ë¥˜ì˜ ê¸°ì—¬ë¥¼ í™˜ì˜í•©ë‹ˆë‹¤!

## ðŸ“© ë¬¸ì˜í•˜ê¸° ðŸ’Œ
ë” ë§Žì€ ì •ë³´ê°€ í•„ìš”í•˜ê±°ë‚˜ í˜‘ë ¥ì„ í¬ë§í•˜ì‹œëŠ” ë¶„ì€ [ì—¬ê¸°](https://github.com/your-github/LLaMA3_Recipes)ë¥¼ í´ë¦­í•˜ì—¬ ì €ì—ê²Œ ë©”ì‹œì§€ë¥¼ ë³´ë‚´ì£¼ì„¸ìš”. í•¨ê»˜ ì§€ì‹ì„ ë‚˜ëˆ„ê³  ì‹¶ìŠµë‹ˆë‹¤!
